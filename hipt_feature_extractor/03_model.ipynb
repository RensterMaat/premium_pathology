{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class DINO_4k_feature_dataset(Dataset):\n",
    "    def __init__(self, target):\n",
    "        features = pd.read_csv('/home/rens/repos/premium_pathology/hipt_feature_extractor/data/features.csv').set_index('Unnamed: 0')\n",
    "        labels = pd.read_csv('/home/rens/repos/premium_pathology/hipt_feature_extractor/data/labels.csv').set_index('Unnamed: 0')\n",
    "        labels.index = [Path(ix).stem for ix in labels.index]\n",
    "\n",
    "        self.target = target\n",
    "\n",
    "        self.data = features.join(labels[target], on='slide')\n",
    "        self.data[target] = self.data[target].astype(int)\n",
    "\n",
    "        self.slides = self.data.slide.unique()\n",
    "        \n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.float()[0])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slides)\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        slide = self.slides[ix]\n",
    "\n",
    "        slide_data = self.data[self.data.slide == slide]\n",
    "\n",
    "        features = slide_data[[f'feature{f}' for f in range(192)]].to_numpy()\n",
    "        x = self.transforms(features)\n",
    "\n",
    "        y = torch.tensor(slide_data[self.target][0]).float()\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "\n",
    "class CLAM_feature_dataset(Dataset):\n",
    "    def __init__(self, target):\n",
    "        self.features_root = Path('/mnt/hpc/pathology/clam_features/primary_vs_metastasis/pt_files')\n",
    "        self.labels = pd.read_csv('/home/rens/repos/premium_pathology/hipt_feature_extractor/data/labels_clam.csv').set_index('Unnamed: 0')\n",
    "        self.labels.index = [Path(ix).stem for ix in self.labels.index]\n",
    "\n",
    "        self.target = target\n",
    "        \n",
    "        self.transforms = transforms.Compose([\n",
    "            # transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.float())\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        x = torch.load(self.features_root / (self.labels.index[ix] + '.pt'))\n",
    "        x = self.transforms(x)\n",
    "\n",
    "        y = self.labels.iloc[ix][self.target]\n",
    "        y = torch.tensor(y).float()\n",
    "\n",
    "        return x, y\n",
    "      \n",
    "        \n",
    "class GatedAttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention_a = [\n",
    "            nn.Linear(input_dim,output_dim),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        self.attention_b = [\n",
    "            nn.Linear(input_dim,output_dim),\n",
    "            nn.Sigmoid()\n",
    "        ]\n",
    "\n",
    "        self.attention_c = [\n",
    "            nn.Linear(output_dim, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        ]\n",
    "\n",
    "        if dropout:\n",
    "            self.attention_a.append(nn.Dropout(dropout))\n",
    "            self.attention_b.append(nn.Dropout(dropout))\n",
    "\n",
    "        self.attention_a = nn.Sequential(*self.attention_a) \n",
    "        self.attention_b = nn.Sequential(*self.attention_b)\n",
    "        self.attention_c = nn.Sequential(*self.attention_c)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = self.attention_a(x)\n",
    "        b = self.attention_b(x)\n",
    "        A = a.mul(b)\n",
    "        out = self.attention_c(A)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class AttentionModel(pl.LightningModule):\n",
    "    def __init__(self, dataset, dropout=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.attention_layer = GatedAttentionLayer(1024, 128, dropout)\n",
    "        self.classifier = nn.Sequential(*[\n",
    "            nn.Linear(1024, 1),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(128,128),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(128,1),\n",
    "            nn.Sigmoid()\n",
    "        ])\n",
    "\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        A = self.attention_layer(x).transpose(1,2)\n",
    "        M = torch.matmul(A, x)\n",
    "        out = self.classifier(M)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "\n",
    "        loss = self.criterion(y_hat.squeeze(), y.squeeze())\n",
    "\n",
    "        self.log('train_loss', loss)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "\n",
    "        loss = self.criterion(y_hat.squeeze(), y.squeeze())\n",
    "\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=5e-4)\n",
    "        return optimizer\n",
    "    \n",
    "    def setup(self, **kwargs):\n",
    "        n = len(self.dataset)\n",
    "        train_n = int(n*0.8)\n",
    "\n",
    "        self.train_dataset, self.val_dataset = random_split(self.dataset, [train_n, n-train_n])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size = 1)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size = 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rens/miniconda3/envs/rens/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/rens/miniconda3/envs/rens/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                | Params\n",
      "--------------------------------------------------------\n",
      "0 | attention_layer | GatedAttentionLayer | 262 K \n",
      "1 | classifier      | Sequential          | 1.0 K \n",
      "2 | criterion       | BCELoss             | 0     \n",
      "--------------------------------------------------------\n",
      "263 K     Trainable params\n",
      "0         Non-trainable params\n",
      "263 K     Total params\n",
      "1.054     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c42a6c4e23ed449cb1d643625563b5dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rens/miniconda3/envs/rens/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/tmp/ipykernel_664/2385727307.py:65: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  y = torch.tensor(y).float()\n",
      "/home/rens/miniconda3/envs/rens/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d193cdbc082042af9df2e499a5daa405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "dataset = CLAM_feature_dataset(target='primary')\n",
    "model = AttentionModel(dataset)\n",
    "\n",
    "logger = WandbLogger(project='pathology', name='debug_attention_on_clam_features')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    accumulate_grad_batches=64,\n",
    "    logger=logger\n",
    "    # overfit_batches=10\n",
    ")\n",
    "\n",
    "trainer.fit(model)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▅▅▅▅██▁▁▁▁▁▅▅▅▅</td></tr><tr><td>train_loss</td><td>▄▇▇▅▃▇▂▃▄▁▂▇▄█▄▄█▇▅</td></tr><tr><td>trainer/global_step</td><td>▁▂▃▃▄▅▆▆▇█▁▁▂▃▃▄▅▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>train_loss</td><td>0.67373</td></tr><tr><td>trainer/global_step</td><td>399</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">debug_attention_on_hipt</strong>: <a href=\"https://wandb.ai/premium/pathology/runs/3ktkn0bw\" target=\"_blank\">https://wandb.ai/premium/pathology/runs/3ktkn0bw</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230317_160219-3ktkn0bw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = model.train_dataloader()\n",
    "\n",
    "for batch in dl:\n",
    "    x, y = batch\n",
    "    y_hat = model(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(x.numpy().flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 81, 192])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
